use std::collections::HashMap;

use once_cell::sync::Lazy;
use serde::Deserialize;
use tokenizers::normalizers::Replace;
use tokenizers::{DecoderWrapper, Tokenizer};

use crate::{Error, Result};

/// GPT2-like tokenizers have multibyte tokens that can have a mix of full and incomplete
/// UTF-8 characters, for example, byte ` \xf0` can be one token. These tokenizers map each
/// byte to a valid UTF-8 character, `TokenProcessor` of `ByteFallback` level will be used
/// to map back these type of characters into bytes, based on `CHAR_MAP`.
///
/// "Ä O" = [U+0120, U+004F] should be interpreted as [0x20, 0x4F] = " O"
/// or
/// "Ä al" = [U+0120, U+0061, U+006C] should be interpreted as [0x20, 0x61, 0x6C] = " al"
///
/// We'll use the following the mapping for this transition:
/// ---
/// 'Ä€' == '\u{0100}' -> 0x00 == 0
/// 'Ä' == '\u{0101}' -> 0x01 == 1
/// 'Ä‚' == '\u{0102}' -> 0x02 == 2
/// ...
/// 'Ä' == '\u{011E}' -> 0x1E == 30
/// 'ÄŸ' == '\u{011F}' -> 0x1F == 31
/// 'Ä ' == '\u{0120}' -> 0x20 == 32
/// ---
/// '!' == '\u{0021}' -> 0x21 == 33
/// '"' == '\u{0022}' -> 0x22 == 34
/// '#' == '\u{0023}' -> 0x23 == 35
/// ...
/// '|' == '\u{007C}' -> 0x7C == 124
/// '}' == '\u{007D}' -> 0x7D == 125
/// '~' == '\u{007E}' -> 0x7E == 126
/// ---
/// 'Ä¡' == '\u{0121}' -> 0x7F == 127
/// 'Ä¢' == '\u{0122}' -> 0x80 == 128
/// 'Ä£' == '\u{0123}' -> 0x81 == 129
/// ...
/// 'Å€' == '\u{0140}' -> 0x9E == 158
/// 'Å' == '\u{0141}' -> 0x9F == 159
/// 'Å‚' == '\u{0142}' -> 0xA0 == 160
/// ---
/// 'Â¡' == '\u{00A1}' -> 0xA1 == 161
/// 'Â¢' == '\u{00A2}' -> 0xA2 == 162
/// 'Â£' == '\u{00A3}' -> 0xA3 == 163
/// ...
/// 'Âª' == '\u{00AA}' -> 0xAA == 170
/// 'Â«' == '\u{00AB}' -> 0xAB == 171
/// 'Â¬' == '\u{00AC}' -> 0xAC == 172
/// ---
/// 'Åƒ' == '\u{0143}' -> 0xAD == 173
/// ---
/// 'Â®' == '\u{00AE}' -> 0xAE == 174
/// 'Â¯' == '\u{00AF}' -> 0xAF == 175
/// 'Â°' == '\u{00B0}' -> 0xB0 == 176
/// ...
/// 'Ã½' == '\u{00FD}' -> 0xFD == 253
/// 'Ã¾' == '\u{00FE}' -> 0xFE == 254
/// 'Ã¿' == '\u{00FF}' -> 0xFF == 255
/// ---
static CHAR_MAP: Lazy<HashMap<char, u8>> = Lazy::new(|| {
    let mut char_map = HashMap::with_capacity(256);
    let mut key = 0x100u32;
    for byte in 0..=255u8 {
        let char = byte as char;
        if matches!(
            char, '!'..='~' | '\u{00A1}'..='\u{00AC}' | '\u{00AE}'..='\u{00FF}',
        ) {
            char_map.insert(char, byte);
        } else if let Some(ch) = char::from_u32(key) {
            char_map.insert(ch, byte);
            key += 1;
        }
    }
    char_map
});

/// Token processor to adjust tokens according to the tokenizer's level.
#[derive(Debug)]
pub(crate) struct TokenProcessor {
    level: TokenProcessorLevel,
}

/// Recognizes different tokenizer's levels.
#[derive(Debug, Clone, PartialEq)]
pub(crate) enum TokenProcessorLevel {
    /// Matches byte level tokenizer (e.g., gpt2).
    Byte,
    /// Matches byte fallback tokenizer (e.g., llama), which have <0x__> tokens for
    /// all __ >= 0x80 to represent incomplete UTF-8 sequences.
    ByteFallback(Mods),
}

impl std::fmt::Display for TokenProcessorLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            Self::Byte => write!(f, "Byte Level"),
            Self::ByteFallback(mods) => write!(f, "Byte Fallback Level with mods: {:?}", mods),
        }
    }
}

/// Modifications to be applied by `TokenProcessor`of `ByteFallback` level.
#[derive(Debug, Clone, PartialEq)]
pub(crate) struct Mods {
    spacechar: char,
}

/// Default string modification to be applied by `TokenProcessor` of `ByteFallback` level.
static DEFAULT_MODS: Mods = Mods { spacechar: ' ' };

impl Mods {
    /// Apply default modifications.
    fn apply_default(&self, token: String) -> String {
        let to = DEFAULT_MODS.spacechar.to_string();
        token.replace(self.spacechar, &to)
    }
}

/// Local structure to be deserialized into from HF's `ReplaceDecoder` in order to get a replace pattern.
#[derive(Debug, Deserialize)]
struct ReplaceDecoder {
    content: String,
    pattern: ReplacePattern,
}

impl ReplaceDecoder {
    fn space_replacement(&self) -> Option<char> {
        if self.content != " " {
            return None;
        }
        match &self.pattern {
            ReplacePattern::String(pattern) => {
                let mut chars = pattern.chars();
                let char = chars.next();
                if let Some(replacement) = char {
                    if chars.next().is_none() {
                        return Some(replacement);
                    }
                }
                None
            }
        }
    }
}

#[derive(Debug, Deserialize)]
enum ReplacePattern {
    String(String),
}

impl TokenProcessor {
    /// Create new `TokenProcessor` with the level defined based on tokenizer's decoders.
    pub(crate) fn new(tokenizer: &Tokenizer) -> Result<Self> {
        match tokenizer.get_decoder() {
            None => Err(Error::UnsupportedByTokenProcessor),
            Some(decoder) => match decoder {
                DecoderWrapper::ByteLevel(_) => Ok(Self {
                    level: TokenProcessorLevel::Byte,
                }),
                DecoderWrapper::Sequence(decoding_sequence) => {
                    let mut is_byte_fallback = false;
                    let mut spacechar = ' ';

                    for decoder in decoding_sequence.get_decoders() {
                        match decoder {
                            DecoderWrapper::ByteFallback(_) => {
                                is_byte_fallback = true;
                            }
                            DecoderWrapper::Replace(replace) => {
                                // `Replace` decoder would replace a pattern in the output with something else,
                                // which we need to know.
                                let decoder = Self::unpack_decoder(replace)?;
                                if let Some(replacement) = decoder.space_replacement() {
                                    spacechar = replacement;
                                }
                            }
                            _ => {}
                        }
                    }

                    if is_byte_fallback {
                        Ok(Self {
                            level: TokenProcessorLevel::ByteFallback(Mods { spacechar }),
                        })
                    } else {
                        Err(Error::UnsupportedByTokenProcessor)
                    }
                }
                _ => Err(Error::UnsupportedByTokenProcessor),
            },
        }
    }

    /// Operates on each token based on the level of `TokenProcessor`.
    pub(crate) fn process(&self, token: String) -> Result<Vec<u8>> {
        match &self.level {
            TokenProcessorLevel::Byte => token
                .chars()
                .map(|char| {
                    CHAR_MAP
                        .get(&char)
                        .copied()
                        .ok_or(Error::ByteProcessorFailed)
                })
                .collect(),
            TokenProcessorLevel::ByteFallback(mods) => {
                // If the token is of form `<0x__>`:
                if token.len() == 6 && token.starts_with("<0x") && token.ends_with('>') {
                    // Get to a single byte specified in the __ part and parse it in base 16 to a byte.
                    match u8::from_str_radix(&token[3..5], 16) {
                        Ok(byte) => Ok([byte].to_vec()),
                        Err(_) => Err(Error::ByteFallbackProcessorFailed),
                    }
                } else {
                    Ok(mods.apply_default(token).as_bytes().to_vec())
                }
            }
        }
    }

    /// Since all fields of HF's `Replace` are private with no getters, it needs to be unpacked
    /// into local `ReplaceDecoder` structure.
    fn unpack_decoder(decoder: &Replace) -> Result<ReplaceDecoder> {
        match serde_json::to_value(decoder) {
            Err(_) => Err(Error::DecoderUnpackingFailed),
            Ok(value) => match serde_json::from_value(value) {
                Ok(d) => Ok(d),
                Err(_) => Err(Error::DecoderUnpackingFailed),
            },
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn byte_level_processor() {
        let model = "openai-community/gpt2";
        let tokenizer = Tokenizer::from_pretrained(model, None).expect("Tokenizer failed");
        let processor = TokenProcessor::new(&tokenizer).expect("Processor failed");

        assert_eq!(processor.level, TokenProcessorLevel::Byte);

        for (ch, byte) in [
            ('Ä€', 0x00),
            ('Ä', 0x01),
            ('Ä‚', 0x02),
            ('Ä', 0x1E),
            ('ÄŸ', 0x1F),
            ('Ä ', 0x20),
            ('!', 0x21),
            ('"', 0x22),
            ('#', 0x23),
            ('|', 0x7C),
            ('}', 0x7D),
            ('~', 0x7E),
            ('Ä¡', 0x7F),
            ('Ä¢', 0x80),
            ('Ä£', 0x81),
            ('Å€', 0x9E),
            ('Å', 0x9F),
            ('Å‚', 0xA0),
            ('Â¡', 0xA1),
            ('Â¢', 0xA2),
            ('Â£', 0xA3),
            ('Âª', 0xAA),
            ('Â«', 0xAB),
            ('Â¬', 0xAC),
            ('Åƒ', 0xAD),
            ('Â®', 0xAE),
            ('Â¯', 0xAF),
            ('Â°', 0xB0),
            ('Ã½', 0xFD),
            ('Ã¾', 0xFE),
            ('Ã¿', 0xFF),
        ] {
            let processed = processor.process(ch.to_string()).expect("Not processed");
            assert_eq!(processed, [byte]);
        }
    }

    #[test]
    fn byte_fallback_level_processor() {
        let model = "hf-internal-testing/llama-tokenizer";
        let tokenizer = Tokenizer::from_pretrained(model, None).expect("Tokenizer failed");
        let processor = TokenProcessor::new(&tokenizer).expect("Processor failed");
        let spacechar = 'â–';
        let mods = Mods { spacechar };

        assert_eq!(processor.level, TokenProcessorLevel::ByteFallback(mods));

        for (input, expected) in [
            ("abc", vec![0x61, 0x62, 0x63]),
            ("<0x61>", vec![0x61]),
            ("<0x61>a", vec![0x3C, 0x30, 0x78, 0x36, 0x31, 0x3E, 0x61]),
            (&spacechar.to_string(), vec![0x20]),
            (
                &format!("{}{}abc", spacechar, spacechar),
                vec![0x20, 0x20, 0x61, 0x62, 0x63],
            ),
            (
                &format!("{}{}{}", spacechar, spacechar, spacechar),
                vec![0x20, 0x20, 0x20],
            ),
        ] {
            let processed = processor.process(input.to_string()).expect("Not processed");
            assert_eq!(processed, expected);
        }
    }

    #[test]
    fn unsupported_tokenizer_error() {
        let model = "hf-internal-testing/tiny-random-XLMRobertaXLForCausalLM";
        let tokenizer = Tokenizer::from_pretrained(model, None).expect("Tokenizer failed");

        let result = TokenProcessor::new(&tokenizer);
        assert!(result.is_err());
        if let Err(e) = result {
            assert_eq!(e, Error::UnsupportedByTokenProcessor)
        }
    }

    #[test]
    fn byte_processor_error() {
        let model = "openai-community/gpt2";
        let tokenizer = Tokenizer::from_pretrained(model, None).expect("Tokenizer failed");
        let processor = TokenProcessor::new(&tokenizer).expect("Processor failed");

        for token in ["ğ’œğ’·ğ’¸ğ’Ÿğ“”", "ğŸ¦„ğŸŒˆğŸŒğŸ”¥ğŸ‰", "äº¬ä¸œè´­ç‰©"] {
            let result = processor.process(token.to_string());
            assert!(result.is_err());
            if let Err(e) = result {
                assert_eq!(e, Error::ByteProcessorFailed)
            }
        }
    }

    #[test]
    fn byte_fallback_processor_error() {
        let model = "hf-internal-testing/llama-tokenizer";
        let tokenizer = Tokenizer::from_pretrained(model, None).expect("Tokenizer failed");
        let processor = TokenProcessor::new(&tokenizer).expect("Processor failed");

        let result = processor.process("<0x6y>".to_string());
        assert!(result.is_err());
        if let Err(e) = result {
            assert_eq!(e, Error::ByteFallbackProcessorFailed)
        }
    }
}
